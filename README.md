# LeetCode Solutions and AI Model Analyzer

üöÄ LLaMA 4 is here‚Äîand it‚Äôs bringing back memories.

Just a week ago, LLaMA 4 was released, and it immediately reminded me of the early days when the LLaMA series set out to bridge the gap between open-source and proprietary LLMs. Although the LLaMA-4-Maverick-17B-128E-Instruct model has dropped in ranking from 2nd to 32nd (likely due to evolving benchmarks and newer versions), I‚Äôm still especially excited to experiment with LLaMA 4 Maverick.

I first noticed support via the Together.ai API, but‚Äîas usual‚ÄîOpenRouter was lightning fast in integrating the model too. With its clean interface and broad model support (from DeepSeek to Gemini 2.5 Pro), OpenRouter has become my go-to for testing.

üîç Before diving into some toy benchmarking experiments, here are a few notable LLaMA 4 advances worth highlighting:

LLaMA 4 Scout supports 10 million token context windows, enabled by interleaved attention layers (iRope)‚Äîremoving the need for positional embeddings entirely.

For those curious about the underlying theory, the paper "Scalable-Softmax is Superior for Attention" is a must-read. In short, scalable-softmax helps:

Speed up pretraining convergence

Generalize better to longer contexts

Improve performance on Needle-in-a-Haystack tasks

üß™ Toy Benchmarking Repo

I built a small repo to evaluate how various LLMs handle LeetCode-style problems. Each script includes:

Code generation

Detailed test case outputs

Explanation logging

Basic performance statistics

Key insight: Most modern LLMs breeze through these tasks‚Äîachieving over 82% success in a single pass. LeetCode problems might no longer be the real test for top-tier models.

üéÆ Fun use-case testing: Mini Games

Both DeepSeek-V3 0324 and LLaMA 4 Maverick performed well on simple tasks like:

üèÉ Endless Runner Game: I preferred DeepSeek‚Äôs version‚Äîit even changes the background color when the game ends.

üîÑ Bounce Ball Game: DeepSeek followed spatial constraints better (balls inside the rotating hexagon), while LLaMA 4 sometimes placed them outside.

üåÄ Mandelbrot Set Visualization: Both models produced visually accurate and smooth outputs.

üí° Overall, LLaMA 4 continues to impress with its technical innovation and usability. If you're experimenting with it too‚Äîcurious to hear your thoughts!

#LLaMA4 #OpenSourceLLM #DeepSeek #OpenRouter #AI #Maverick #Benchmarking #GenerativeAI #LLM #LeetCode #Mandelbrot #Research

## Quick Links

- [Online LLM Leetcode Comparison Report](https://donghaozhang.github.io/Leetcode_play/llm_analysis_result/models_comparison_report.html) - View LLM Leetcode comparison results
![LeetCode Models Comparison Results](llm_analysis_result/leetcode_page.png)
- [Online LLM Game Comparison Report](https://donghaozhang.github.io/Leetcode_play/llm_game_compare/video_gallery.html) - View LLM game comparison results
![Online LLM Game Comparison Report](llm_game_compare/llm_game.png)
- [LeetCode Solutions Documentation](leetcode_questions/README.md) - Full list of solutions organized by category

## Project Structure

- `leetcode_questions/` - Contains all LeetCode problem solutions organized by category
- `llm_analysis_result/` - Contains HTML reports comparing different AI model solutions
- `deepseek_solutions/`, `llama4_maverick_solutions/`, `gemini_solutions/` - Solutions generated by different AI models
- `solver_scripts/` - Contains scripts for generating solutions using different AI models
- `llm_leetcode_analyzer.py` - Main analyzer script
- `serve.py` - Local web server for viewing the analysis reports

## LeetCode Solution Analyzer

### Overview

The LeetCode Solution Analyzer is a Python-based tool that:
- Analyzes Python solutions for LeetCode problems
- Generates detailed HTML reports with statistics
- Tracks solution status (Pass/Fail/Unsolved)
- Provides links to solution files
- Shows error details for failed solutions
- Supports multiple AI model solutions

### Usage

#### Running the Analyzer
```bash
python llm_leetcode_analyzer.py
```

#### Viewing the Reports
- **Online version**: [GitHub Pages Report](https://donghaozhang.github.io/Leetcode_play/llm_analysis_result/models_comparison_report.html)
- **Local version**:
  1. Run `python serve.py` to start the local server
  2. The browser will automatically open to the comparison report
  3. Or navigate to: [http://localhost:8000/llm_analysis_result/models_comparison_report.html](http://localhost:8000/llm_analysis_result/models_comparison_report.html)

#### Generating Solutions
```bash
python solver_scripts/deepseek_leetcode_solver.py
python solver_scripts/llama4_leetcode_solver.py
python solver_scripts/gemini_leetcode_solver.py
```

### Analysis Features
- Automatic solution verification
- Error detection and reporting
- Creation time tracking
- Problem status classification
- Detailed error messages

### Report Features
- Interactive HTML interface
- Model-specific tabs
- Real-time statistics
- Solution file links
- Error detail toggles
- Responsive design

## Requirements

- Python 3.x
- Standard Python libraries:
  - os
  - subprocess
  - re
  - datetime
  - webbrowser
  - requests
  - dotenv

## Core Components

### Solution Analyzers
- **llm_leetcode_analyzer.py**: Main script that analyzes solutions and generates reports
- **deepseek_leetcode_solver.py**: Generates solutions using DeepSeek model
- **llama4_leetcode_solver.py**: Generates solutions using Llama-4 Maverick model
- **gemini_leetcode_solver.py**: Generates solutions using Gemini 2.5 Pro model

### HTML Reports
- **models_comparison_report.html**: Compares solutions from all models
- **deepseek_solution_report.html**: DeepSeek model solutions and statistics
- **llama4_maverick_solution_report.html**: Llama-4 model solutions and statistics
- **gemini_solution_report.html**: Gemini model solutions and statistics

## Contributing

Feel free to submit issues and enhancement requests!

## License

This project is open source and available under the MIT License.
